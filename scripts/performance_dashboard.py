#!/usr/bin/env python3
"""
ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ
"""
import json
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import numpy as np

class PerformanceDashboard:
    """ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 metrics_file: str = "docs/CURRENT/context_metrics.json",
                 ab_test_file: str = "docs/CURRENT/ab_test_results.json"):
        self.metrics_file = Path(metrics_file)
        self.ab_test_file = Path(ab_test_file)
        
    def generate_report(self, output_file: str = "docs/CURRENT/performance_report.md") -> Dict:
        """ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # ë°ì´í„° ë¡œë“œ
        performance_data = self._load_performance_data()
        ab_test_data = self._load_ab_test_data()
        
        # ë¶„ì„ ìˆ˜í–‰
        performance_analysis = self._analyze_performance(performance_data)
        ab_test_analysis = self._analyze_ab_test(ab_test_data)
        recommendations = self._generate_recommendations(performance_analysis, ab_test_analysis)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = self._generate_markdown_report(performance_analysis, ab_test_analysis, recommendations)
        
        # íŒŒì¼ ì €ì¥
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        return {
            "performance_score": performance_analysis.get("overall_score", 0),
            "ab_test_winner": ab_test_analysis.get("winner", "inconclusive"),
            "recommendation": recommendations.get("primary_action", "continue_monitoring"),
            "report_file": output_file
        }
    
    def _load_performance_data(self) -> Dict:
        """ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ"""
        if not self.metrics_file.exists():
            return {}
        try:
            with open(self.metrics_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            return {}
    
    def _load_ab_test_data(self) -> Dict:
        """A/B í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        if not self.ab_test_file.exists():
            return {}
        try:
            with open(self.ab_test_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            return {}
    
    def _analyze_performance(self, data: Dict) -> Dict:
        """ì„±ëŠ¥ ë°ì´í„° ë¶„ì„"""
        if not data:
            return {"error": "No performance data available"}
        
        # ìµœê·¼ ë©”íŠ¸ë¦­ ê³„ì‚°
        recent_metrics = list(data.values())[-20:]  # ìµœê·¼ 20ê°œ
        
        # íš¨ìœ¨ì„± ë©”íŠ¸ë¦­
        token_improvements = []
        response_times = []
        success_rates = []
        
        for metric in recent_metrics:
            if metric.get('token_count_before') and metric.get('token_count_after'):
                improvement = (metric['token_count_before'] - metric['token_count_after']) / metric['token_count_before'] * 100
                token_improvements.append(improvement)
            
            if metric.get('response_time_ms'):
                response_times.append(metric['response_time_ms'])
            
            success_rates.append(1.0 if metric.get('first_attempt_success') else 0.0)
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­
        quality_issues = [
            metric for metric in recent_metrics 
            if (metric.get('context_miss_detected') or 
                metric.get('duplicate_work_detected') or
                metric.get('consistency_violation'))
        ]
        
        return {
            "sample_size": len(recent_metrics),
            "avg_token_improvement": np.mean(token_improvements) if token_improvements else 0,
            "avg_response_time": np.mean(response_times) if response_times else 0,
            "success_rate": np.mean(success_rates) * 100,
            "quality_issue_rate": len(quality_issues) / len(recent_metrics) * 100,
            "overall_score": self._calculate_overall_score(token_improvements, success_rates, quality_issues, recent_metrics),
            "trend": self._analyze_trend(recent_metrics)
        }
    
    def _analyze_ab_test(self, data: Dict) -> Dict:
        """A/B í…ŒìŠ¤íŠ¸ ë¶„ì„"""
        if not data:
            return {"error": "No A/B test data available"}
        
        control_sessions = [s for s in data.values() if s["group"] == "control"]
        treatment_sessions = [s for s in data.values() if s["group"] == "treatment"]
        
        if len(control_sessions) < 5 or len(treatment_sessions) < 5:
            return {"error": "Insufficient data for A/B analysis"}
        
        control_stats = self._calculate_group_stats(control_sessions)
        treatment_stats = self._calculate_group_stats(treatment_sessions)
        
        # ìŠ¹ì ê²°ì •
        winner = "inconclusive"
        if treatment_stats["success_rate"] > control_stats["success_rate"] * 1.05:  # 5% ì´ìƒ ê°œì„ 
            if treatment_stats["avg_completion_time"] <= control_stats["avg_completion_time"] * 1.1:  # ì‹œê°„ì€ 10% ì´ë‚´ ì¦ê°€ í—ˆìš©
                winner = "treatment"
        elif control_stats["success_rate"] > treatment_stats["success_rate"] * 1.05:
            winner = "control"
        
        return {
            "control_stats": control_stats,
            "treatment_stats": treatment_stats,
            "winner": winner,
            "confidence": self._calculate_confidence(control_sessions, treatment_sessions)
        }
    
    def _generate_recommendations(self, performance: Dict, ab_test: Dict) -> Dict:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì„±ëŠ¥ ê¸°ë°˜ ì¶”ì²œ
        if performance.get("overall_score", 0) > 75:
            if performance.get("success_rate", 0) > 70:
                recommendations.append("ì„±ëŠ¥ ê¸°ì¤€ í†µê³¼: ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ê°€ëŠ¥")
            else:
                recommendations.append("ì„±ëŠ¥ì€ ì¢‹ìœ¼ë‚˜ ì„±ê³µë¥  ê°œì„  í•„ìš”")
        else:
            recommendations.append("ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ë‹¬: ì‹œìŠ¤í…œ ì¡°ì • í•„ìš”")
        
        # A/B í…ŒìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì²œ
        ab_winner = ab_test.get("winner", "inconclusive")
        if ab_winner == "treatment":
            recommendations.append("Treatment group ìš°ì„¸: ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ í™•ëŒ€ ê¶Œì¥")
        elif ab_winner == "control":
            recommendations.append("Control group ìš°ì„¸: ê¸°ì¡´ ë°©ì‹ ìœ ì§€ ê¶Œì¥")
        else:
            recommendations.append("A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶ˆë¶„ëª…: ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ í•„ìš”")
        
        # ì£¼ìš” ì•¡ì…˜ ê²°ì •
        primary_action = "continue_monitoring"
        if performance.get("overall_score", 0) > 80 and ab_winner == "treatment":
            primary_action = "expand_rollout"
        elif performance.get("overall_score", 0) < 60:
            primary_action = "rollback_changes"
        
        return {
            "recommendations": recommendations,
            "primary_action": primary_action
        }
    
    def _calculate_overall_score(self, token_improvements, success_rates, quality_issues, all_metrics) -> float:
        """ì „ì²´ ì ìˆ˜ ê³„ì‚°"""
        if not all_metrics:
            return 0
        
        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        efficiency_score = np.mean(token_improvements) if token_improvements else 0
        quality_score = np.mean(success_rates) * 100
        reliability_score = max(0, 100 - (len(quality_issues) / len(all_metrics) * 100))
        
        return (efficiency_score * 0.3 + quality_score * 0.5 + reliability_score * 0.2)
    
    def _analyze_trend(self, metrics: List[Dict]) -> str:
        """íŠ¸ë Œë“œ ë¶„ì„"""
        if len(metrics) < 10:
            return "insufficient_data"
        
        recent_scores = []
        for metric in metrics[-10:]:
            # ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚°
            score = 0
            if metric.get('first_attempt_success'):
                score += 50
            if not metric.get('context_miss_detected'):
                score += 25
            if not metric.get('duplicate_work_detected'):
                score += 25
            recent_scores.append(score)
        
        # ì„ í˜• íšŒê·€ë¡œ íŠ¸ë Œë“œ ê³„ì‚°
        x = np.arange(len(recent_scores))
        slope = np.polyfit(x, recent_scores, 1)[0]
        
        if slope > 2:
            return "improving"
        elif slope < -2:
            return "declining"
        else:
            return "stable"
    
    def _calculate_group_stats(self, sessions: List[Dict]) -> Dict:
        """ê·¸ë£¹ í†µê³„ ê³„ì‚°"""
        completed = [s for s in sessions if s.get("completion_time_minutes") is not None]
        
        if not completed:
            return {"error": "No completed sessions"}
        
        return {
            "total_sessions": len(sessions),
            "completed_sessions": len(completed),
            "success_rate": sum(1 for s in completed if s.get("success_achieved")) / len(completed) * 100,
            "avg_completion_time": sum(s["completion_time_minutes"] for s in completed) / len(completed),
            "consistency_rate": sum(1 for s in completed if s.get("consistency_maintained")) / len(completed) * 100
        }
    
    def _calculate_confidence(self, control: List[Dict], treatment: List[Dict]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
        control_size = len([s for s in control if s.get("completion_time_minutes")])
        treatment_size = len([s for s in treatment if s.get("completion_time_minutes")])
        
        if control_size >= 30 and treatment_size >= 30:
            return 0.95
        elif control_size >= 15 and treatment_size >= 15:
            return 0.80
        elif control_size >= 10 and treatment_size >= 10:
            return 0.65
        else:
            return 0.50
    
    def _generate_markdown_report(self, performance: Dict, ab_test: Dict, recommendations: Dict) -> str:
        """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""# ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¦¬í¬íŠ¸

## ğŸ“… ë¦¬í¬íŠ¸ ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š ì „ì²´ ì„±ëŠ¥ ìš”ì•½

### ì¢…í•© ì ìˆ˜: {performance.get('overall_score', 0):.1f}/100

### ì£¼ìš” ë©”íŠ¸ë¦­
- **ì„±ê³µë¥ **: {performance.get('success_rate', 0):.1f}%
- **í† í° íš¨ìœ¨ì„±**: {performance.get('avg_token_improvement', 0):.1f}% ê°œì„ 
- **í’ˆì§ˆ ì´ìŠˆìœ¨**: {performance.get('quality_issue_rate', 0):.1f}%
- **íŠ¸ë Œë“œ**: {performance.get('trend', 'unknown')}

## ğŸ§ª A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼

### ìŠ¹ì: {ab_test.get('winner', 'inconclusive')}

#### Control Group (ê¸°ì¡´ ë°©ì‹)
{self._format_group_stats(ab_test.get('control_stats', {}))}

#### Treatment Group (ìƒˆ ë°©ì‹)  
{self._format_group_stats(ab_test.get('treatment_stats', {}))}

### ì‹ ë¢°ë„: {ab_test.get('confidence', 0):.0%}

## ğŸ’¡ ì¶”ì²œì‚¬í•­

"""
        
        for i, rec in enumerate(recommendations.get('recommendations', []), 1):
            report += f"{i}. {rec}\n"
        
        report += f"""
## ğŸ¯ ì£¼ìš” ì•¡ì…˜

**{recommendations.get('primary_action', 'continue_monitoring')}**

## ğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„

"""
        
        primary_action = recommendations.get('primary_action', 'continue_monitoring')
        if primary_action == "expand_rollout":
            report += "- ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì‹œìŠ¤í…œì„ ë” ë§ì€ ì‚¬ìš©ìì—ê²Œ í™•ëŒ€\n- ëª¨ë‹ˆí„°ë§ ì§€ì†\n- ì„±ëŠ¥ ì„ê³„ê°’ ì¬ì¡°ì •"
        elif primary_action == "rollback_changes":
            report += "- ì¦‰ì‹œ ê¸°ì¡´ ì‹œìŠ¤í…œìœ¼ë¡œ ë¡¤ë°±\n- ë¬¸ì œì  ë¶„ì„ ë° ê°œì„ \n- ì¬í…ŒìŠ¤íŠ¸ ê³„íš ìˆ˜ë¦½"  
        else:
            report += "- í˜„ì¬ ìƒíƒœ ìœ ì§€í•˜ë©° ë°ì´í„° ìˆ˜ì§‘ ì§€ì†\n- ì£¼ê°„ ì„±ê³¼ ê²€í† \n- ì„ê³„ê°’ ë„ë‹¬ ì‹œ ì¬í‰ê°€"
        
        return report
    
    def _format_group_stats(self, stats: Dict) -> str:
        """ê·¸ë£¹ í†µê³„ í¬ë§·íŒ…"""
        if not stats or "error" in stats:
            return "- ë°ì´í„° ë¶€ì¡±"
        
        return f"""- ì™„ë£Œ ì„¸ì…˜: {stats.get('completed_sessions', 0)}ê°œ
- ì„±ê³µë¥ : {stats.get('success_rate', 0):.1f}%  
- í‰ê·  ì™„ë£Œ ì‹œê°„: {stats.get('avg_completion_time', 0):.1f}ë¶„
- ì¼ê´€ì„± ìœ ì§€ìœ¨: {stats.get('consistency_rate', 0):.1f}%"""

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    dashboard = PerformanceDashboard()
    result = dashboard.generate_report()
    
    print(f"ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {result['report_file']}")
    print(f"ì „ì²´ ì ìˆ˜: {result['performance_score']:.1f}")
    print(f"A/B í…ŒìŠ¤íŠ¸ ìŠ¹ì: {result['ab_test_winner']}")
    print(f"ê¶Œì¥ ì•¡ì…˜: {result['recommendation']}")

if __name__ == "__main__":
    main()